import os
import sys
import torch
import numpy as np
import gradio as gr
import soundfile as sf
import tempfile
import hashlib
import requests
import socket
from huggingface_hub import snapshot_download

# ================= 1. ç¯å¢ƒä¸æ™ºèƒ½åŒæ­¥é€»è¾‘ (æ”¯æŒçº¯ç¦»çº¿) =================
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

def sync_model_files():
    """æ™ºèƒ½åŒæ­¥ï¼šä¼˜å…ˆä¿è¯ç¦»çº¿å¯ç”¨ï¼Œä»…åœ¨åœ¨çº¿ä¸”æ–‡ä»¶ç¼ºå¤±æ—¶å¼ºåˆ¶åŒæ­¥"""
    repo_id = "shawnpi/HQ-SVC"
    
    # å®šä¹‰æ ¸å¿ƒæƒé‡è·¯å¾„ï¼ˆæ ¹æ®ä½ çš„ YAML é…ç½®å¯¹é½ï¼‰
    model_pth = "utils/pretrain/250000_step_val_loss_0.50.pth"
    vocoder_dir = "utils/pretrain/nsf_hifigan/model"
    rmvpe_path = "utils/pretrain/rmvpe/model.pt"
    # æ£€æŸ¥æœ¬åœ°æ ¸å¿ƒæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    local_exists = os.path.exists(model_pth) and os.path.exists(vocoder_dir)
    
    if local_exists:
        print(">>> [ç¦»çº¿æ¨¡å¼] æ£€æµ‹åˆ°æœ¬åœ°æƒé‡å·²å®Œæ•´")
        return

    # å¦‚æœæœ¬åœ°æ–‡ä»¶ç¼ºå¤±ï¼Œåˆ™å°è¯•ç½‘ç»œåŒæ­¥
    print(">>> [åŒæ­¥æ¨¡å¼] æœ¬åœ°æƒé‡ä¸å®Œæ•´ï¼Œæ­£åœ¨æ£€æµ‹ç½‘ç»œä»¥è·å–æƒé‡...")

    try:
        snapshot_download(
            repo_id=repo_id,
            allow_patterns=["utils/pretrain/*", "config.json"],
            local_dir=".",
            local_dir_use_symlinks=False,
            # å¦‚æœä¾ç„¶å¤±è´¥ï¼ˆå¦‚é•œåƒç«™ä¹Ÿè¿ä¸ä¸Šï¼‰ï¼Œåˆ™å°è¯•ä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜
            resume_download=True 
        )
        print(">>> æƒé‡åŒæ­¥å®Œæˆã€‚")
    except Exception as e:
        if local_exists:
            print(f">>> åŒæ­¥å¤±è´¥ä½†æœ¬åœ°å·²æœ‰æ–‡ä»¶ï¼Œå°†å°è¯•ç»§ç»­è¿è¡Œã€‚é”™è¯¯: {e}")
        else:
            print(f">>> [ä¸¥é‡é”™è¯¯] åŒæ­¥å¤±è´¥ä¸”æœ¬åœ°ç¼ºå°‘æƒé‡ï¼Œç¨‹åºå¯èƒ½æ— æ³•è¿è¡Œ: {e}")

# åœ¨ä¸€åˆ‡å¼€å§‹å‰æ‰§è¡Œæ™ºèƒ½åŒæ­¥
sync_model_files()

# ================= 2. è·¯å¾„ä¸æ¨¡å‹åŠ è½½é€»è¾‘ =================
now_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(now_dir)
utils_path = os.path.join(now_dir, 'utils')
if utils_path not in sys.path:
    sys.path.append(utils_path)

from logger.utils import load_config
from utils.models.models_v2_beta import load_hq_svc
from utils.vocoder import Vocoder
from utils.data_preprocessing import load_facodec, load_f0_extractor, load_volume_extractor, get_processed_file

# å…¨å±€å˜é‡ç¼“å­˜
NET_G = None
VOCODER = None
ARGS = None
PREPROCESSORS = {}
TARGET_CACHE = {"file_hash": None, "spk_ave": None, "all_tar_f0": None}

def initialize_models(config_path):
    global NET_G, VOCODER, ARGS, PREPROCESSORS
    ARGS = load_config(config_path)
    ARGS.config = config_path
    device = ARGS.device
    
    # å®ä¾‹åŒ–æ¨¡å‹
    VOCODER = Vocoder(vocoder_type='nsf-hifigan', vocoder_ckpt='utils/pretrain/nsf_hifigan/model', device=device)
    NET_G = load_hq_svc(mode='infer', device=device, model_path=ARGS.model_path, args=ARGS)
    NET_G.eval()
    
    fa_encoder, fa_decoder = load_facodec(device)
    PREPROCESSORS = {
        "fa_encoder": fa_encoder, "fa_decoder": fa_decoder, 
        "f0_extractor": load_f0_extractor(ARGS), 
        "volume_extractor": load_volume_extractor(ARGS),
        "content_encoder": None, "spk_encoder": None
    }

# ================= 3. æ¨ç†é€»è¾‘ (ä¿æŒé²æ£’æ€§) =================
def predict(source_audio, target_files, shift_key, adjust_f0):
    global TARGET_CACHE
    if source_audio is None:
        return "âš ï¸ ç³»ç»Ÿæç¤ºï¼šæœªæ£€æµ‹åˆ°æºéŸ³é¢‘ã€‚è¯·ç¡®ä¿æ–‡ä»¶å·²ä¸Šä¼ å®Œæ¯•ã€‚", None

    if not os.path.exists(source_audio):
        return "âŒ ç³»ç»Ÿé”™è¯¯ï¼šæ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·é‡æ–°ä¸Šä¼ ã€‚", None

    sr, encoder_sr, device = ARGS.sample_rate, ARGS.encoder_sr, ARGS.device

    try:
        with torch.no_grad():
            is_reconstruction = (target_files is None or len(target_files) == 0)
            target_names = "".join([f.name if hasattr(f, 'name') else f for f in (target_files or [])])
            current_hash = hashlib.md5(target_names.encode()).hexdigest()
            
            if is_reconstruction:
                t_data = get_processed_file(source_audio, sr, encoder_sr, VOCODER, PREPROCESSORS["volume_extractor"], PREPROCESSORS["f0_extractor"], PREPROCESSORS["fa_encoder"], PREPROCESSORS["fa_decoder"], None, None, device=device)
                spk_ave, all_tar_f0 = t_data['spk'].squeeze().to(device), t_data['f0_origin']
                status = "âœ¨ Super-Resolution"
            elif TARGET_CACHE["file_hash"] == current_hash:
                spk_ave, all_tar_f0 = TARGET_CACHE["spk_ave"], TARGET_CACHE["all_tar_f0"]
                status = "ğŸš€ Cache Loaded"
            else:
                spk_list, f0_list = [], []
                for f in (target_files[:20] if target_files else []):
                    f_path = f.name if hasattr(f, 'name') else f
                    if not f_path or not os.path.exists(f_path): continue
                    t_data = get_processed_file(f_path, sr, encoder_sr, VOCODER, PREPROCESSORS["volume_extractor"], PREPROCESSORS["f0_extractor"], PREPROCESSORS["fa_encoder"], PREPROCESSORS["fa_decoder"], None, None, device=device)
                    if t_data: 
                        spk_list.append(t_data['spk'])
                        f0_list.append(t_data['f0_origin'])
                
                if not spk_list: return "âŒ ç»ˆç«¯æç¤ºï¼šå‚è€ƒéŸ³é¢‘å¤„ç†å¤±è´¥ã€‚", None
                spk_ave = torch.stack(spk_list).mean(dim=0).squeeze().to(device)
                all_tar_f0 = np.concatenate(f0_list)
                TARGET_CACHE.update({"file_hash": current_hash, "spk_ave": spk_ave, "all_tar_f0": all_tar_f0})
                status = "âœ… VOICE CONVERSION"

            src_data = get_processed_file(source_audio, sr, encoder_sr, VOCODER, PREPROCESSORS["volume_extractor"], PREPROCESSORS["f0_extractor"], PREPROCESSORS["fa_encoder"], PREPROCESSORS["fa_decoder"], None, None, device=device)
            f0 = src_data['f0'].unsqueeze(0).to(device)
            
            if adjust_f0 and not is_reconstruction:
                src_f0_valid = src_data['f0_origin'][src_data['f0_origin'] > 0]
                tar_f0_valid = all_tar_f0[all_tar_f0 > 0]
                if len(src_f0_valid) > 0 and len(tar_f0_valid) > 0:
                    shift_key = round(12 * np.log2(tar_f0_valid.mean() / src_f0_valid.mean()))
            
            f0 = f0 * 2 ** (float(shift_key) / 12)
            mel_g = NET_G(src_data['vq_post'].unsqueeze(0).to(device), f0, src_data['vol'].unsqueeze(0).to(device), spk_ave, gt_spec=None, infer=True, infer_speedup=ARGS.infer_speedup, method=ARGS.infer_method, vocoder=VOCODER)
            wav_g = VOCODER.infer(mel_g, f0) if ARGS.vocoder == 'nsf-hifigan' else VOCODER.infer(mel_g)
            
            out_p = tempfile.mktemp(suffix=".wav")
            sf.write(out_p, wav_g.squeeze().cpu().numpy(), 44100)
            return f"{status} | Pitch Shifted: {shift_key}", out_p
    except Exception as e:
        return f"âŒ æ¨ç†è¿è¡Œå‡ºé”™ï¼š{str(e)}", None

custom_css = """
@import url('https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap');
:root { --font: 'Press Start 2P', cursive !important; }
* { font-family: 'Press Start 2P', cursive !important; border-radius: 0px !important; }
.gradio-container {
    background: linear-gradient(rgba(0,0,0,0.85), rgba(0,0,0,0.85)),
                url('https://img.moegirl.org.cn/common/d/d3/K-ON_key_visual_2.jpg');
    background-size: cover;
}
.gr-box, .gr-input, .gr-button { border: 4px solid #000 !important; box-shadow: 8px 8px 0px #000 !important; }
label, p, .time-info { color: #f36c18 !important; font-size: 10px !important; text-transform: uppercase; }
h1 { color: #FFFF00 !important; text-shadow: 4px 4px 0px #000 !important; text-align: center; }
button.primary { background-color: #ff69b4 !important; color: #fff !important; }
footer { display: none !important; }
"""

# ================= 4. UI ç•Œé¢ =================
def build_ui():
    # è·å– GIF çš„ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿ Gradio èƒ½å¤Ÿç²¾å‡†å®šä½æ–‡ä»¶
    gif_path = os.path.join(now_dir, "images", "kon-new.gif")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸æ˜¾ç¤ºæˆ–æ˜¾ç¤ºæ–‡å­—
    if os.path.exists(gif_path):
        # æ³¨æ„ï¼šåœ¨ HTML ä¸­ï¼ŒGradio è¦æ±‚æœ¬åœ°æ–‡ä»¶è·¯å¾„å‰ç¼€ä¸º "file/"
        img_html = f'''
            <div style="display: flex; justify-content: center; margin: 20px 0;">
                <div style="border: 4px solid #000; box-shadow: 8px 8px 0px #000; background: #fff; line-height: 0;">
                    <img src="file/{gif_path}" style="max-width: 400px; width: 100%; display: block;">
                </div>
            </div>
        '''
    else:
        # å…œåº•æ–¹æ¡ˆï¼šå¦‚æœå›¾ç‰‡ä¸¢äº†ï¼Œæ˜¾ç¤ºä¸€ä¸ªåƒç´ é£çš„å ä½ç¬¦
        img_html = '<div style="text-align:center; padding: 20px; color: #FFFF00;">[ ğŸ¸ IMAGE NOT FOUND ]</div>'

    with gr.Blocks(css=custom_css, title="HQ-SVC Pixel Pro") as demo:
        # æ¸²æŸ“ GIF åŒºåŸŸ
        gr.HTML(img_html)
        
        gr.Markdown("# ğŸ¸HQ-SVC: SINGING VOICE CONVERSION AND SUPER-RESOLUTIONğŸ°")
        
        with gr.Row():
            with gr.Column():
                # ä¹‹å‰è®¨è®ºè¿‡çš„ï¼šå¢åŠ æ–‡ä»¶ç±»å‹é™åˆ¶ï¼Œæé«˜é²æ£’æ€§
                src_audio = gr.Audio(label="STEP 1: SOURCE VOICE", type="filepath")
                tar_files = gr.File(label="STEP 2: TARGET REFERENCE", file_count="multiple")
                with gr.Row():
                    key_shift = gr.Number(label="PITCH SHIFT", value=0)
                    auto_f0 = gr.Checkbox(label="AUTO PITCH", value=False)
                run_btn = gr.Button("ğŸ¤ START CONVERSION!", variant="primary")
            
            with gr.Column():
                # ç³»ç»Ÿç»ˆç«¯ï¼šæ˜¾ç¤ºæ¨ç†çŠ¶æ€æˆ–æŠ¥é”™ä¿¡æ¯
                status_box = gr.Textbox(label="SYSTEM TERMINAL", interactive=False, placeholder="Waiting for input...")
                result_audio = gr.Audio(label="OUTPUT (44.1kHz HQ)")

        run_btn.click(predict, [src_audio, tar_files, key_shift, auto_f0], [status_box, result_audio])
        
    return demo

if __name__ == "__main__":
    config_p = "configs/hq_svc_infer.yaml"
    if os.path.exists(config_p):
        initialize_models(config_p)
    
    demo = build_ui()
    temp_dir = tempfile.gettempdir()
    demo.launch(
        share=True,
        allowed_paths=[os.path.join(os.path.dirname(__file__), "images"), os.path.dirname(__file__), temp_dir]
    )
